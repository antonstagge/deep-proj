{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import random\n",
    "import math\n",
    "import re\n",
    "import time\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import skimage.draw\n",
    "import tensorflow as tf\n",
    "\n",
    "# Root directory of the project\n",
    "ROOT_DIR = os.path.abspath(\"../\")\n",
    "\n",
    "# Import Mask RCNN\n",
    "sys.path.append(ROOT_DIR)  # To find local version of the library\n",
    "\n",
    "from mrcnn import utils\n",
    "import mrcnn.model as modellib\n",
    "from mrcnn import visualize\n",
    "from mrcnn.visualize import display_images\n",
    "from mrcnn.model import log\n",
    "\n",
    "#Import adaptions\n",
    "from fashion_config import FashionConfig\n",
    "from fashion_dataset import FashionDataset\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Hide some tensorflaw warning messages\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "# Directory to save logs and trained model\n",
    "MODEL_DIR = os.path.join(ROOT_DIR, \"logs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Load config \"\"\" \n",
    "config = FashionConfig()\n",
    "#config.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Prepare dataset \"\"\" \n",
    "dataset_train = FashionDataset()\n",
    "dataset_train.load_fashion(ROOT_DIR + '/datasets/big_deepfashion2', \"train\")\n",
    "dataset_train.prepare()\n",
    "\n",
    "dataset_val = FashionDataset()\n",
    "dataset_val.load_fashion(ROOT_DIR + '/datasets/big_deepfashion2', \"val\")\n",
    "dataset_val.prepare()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class InferenceConfig(FashionConfig):\n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 1\n",
    "\n",
    "inference_config = InferenceConfig()\n",
    "\n",
    "# Recreate the model in inference mode\n",
    "model = modellib.MaskRCNN(mode=\"inference\", \n",
    "                          config=inference_config,\n",
    "                          model_dir=MODEL_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load trained weights\n",
    "model_path = \"../logs/final_logs/logs_mask_rcnn_fashion_0045.h5\"\n",
    "print(\"Loading weights from \", model_path)\n",
    "model.load_weights(model_path, by_name=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ax(rows=1, cols=1, size=8):\n",
    "    \"\"\"Return a Matplotlib Axes array to be used in\n",
    "    all visualizations in the notebook. Provide a\n",
    "    central point to control graph sizes.\n",
    "    \n",
    "    Change the default size attribute to control the size\n",
    "    of rendered images\n",
    "    \"\"\"\n",
    "    _, ax = plt.subplots(rows, cols, figsize=(size*cols, size*rows))\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EVALUATIONS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visual comparison between ground truth and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on a random image\n",
    "image_id = random.choice(dataset_val.image_ids)\n",
    "\n",
    "#Test on specific image\n",
    "#image_id = 17803\n",
    "#image_id = 7818\n",
    "print(\"Image #{}\".format(image_id))\n",
    "\n",
    "image, image_meta, gt_class_id, gt_bbox, gt_mask, gt_landmark =\\\n",
    "    modellib.load_image_gt(dataset_val, inference_config, \n",
    "                           image_id, use_mini_mask=False)\n",
    "\n",
    "log(\"image\", image)\n",
    "log(\"image_meta\", image_meta)\n",
    "log(\"gt_class_id\", gt_class_id)\n",
    "log(\"gt_bbox\", gt_bbox)\n",
    "log(\"gt_mask\", gt_mask)\n",
    "log(\"gt_landmark\", gt_landmark)\n",
    "\n",
    "visualize.display_instances(image, gt_bbox, gt_mask, gt_landmark, gt_class_id, \n",
    "                            dataset_val.class_names, figsize=(8, 8))\n",
    "\n",
    "results = model.detect([image], verbose=2)\n",
    "\n",
    "r = results[0]\n",
    "visualize.display_instances(image, r['rois'], r['masks'], r['landmarks'], r['class_ids'], \n",
    "                            dataset_val.class_names, r['scores'], ax=get_ax())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision-recall curve\n",
    "* The X-axis is recall: Recall is High if the amount of False Negatives is Low\n",
    "* The Y-axis is precision: Precision is High if the amount of False Positives is low.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw precision-recall curve based on what is returned in the image\n",
    "AP, precisions, recalls, overlaps = utils.compute_ap(gt_bbox, gt_class_id, gt_mask,\n",
    "                                          r['rois'], r['class_ids'], r['scores'], r['masks'])\n",
    "visualize.plot_precision_recall(AP, precisions, recalls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heatmap of predictions and ground truth (Still in beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid of ground truth objects and their predictions\n",
    "visualize.plot_overlaps(gt_class_id, r['class_ids'], r['scores'],\n",
    "                        overlaps, dataset_val.class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare the generated masks\n",
    "Useful to see what the network sees when there is overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize.display_top_masks(image, gt_mask, gt_class_id, \n",
    "                            dataset_val.class_names)\n",
    "visualize.display_top_masks(image, r['masks'], r['class_ids'], \n",
    "                            dataset_val.class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Activations\n",
    "\n",
    "In some cases it helps to look at the output from different layers and visualize them to catch issues and odd patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check the names of all layers\n",
    "for layer in model.keras_model.layers:\n",
    "    print(layer.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get activations of a few sample layers\n",
    "\n",
    "# Select layer to visualize. Most play nicely with the backbone feature mapping, but not all.\n",
    "layer_name = 'res3a_out'\n",
    "\n",
    "activations = model.run_graph([image], [\n",
    "    (layer_name,           model.keras_model.get_layer(layer_name).output),\n",
    "    (\"input_image\",        tf.identity(model.keras_model.get_layer(\"input_image\").output)),\n",
    "    (\"rpn_bbox\",           model.keras_model.get_layer(\"rpn_bbox\").output),\n",
    "    (\"roi\",                model.keras_model.get_layer(\"ROI\").output),\n",
    "])\n",
    "\n",
    "# Show input image (normalized)\n",
    "_ = plt.imshow(modellib.unmold_image(activations[\"input_image\"][0], config))\n",
    "\n",
    "# Show Backbone feature map\n",
    "display_images(np.transpose(activations[layer_name][0,:,:,:16], [2, 0, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Mask Average Precison (mAP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute VOC-Style mAP @ IoU=0.5\n",
    "# Running on 10 images. Increase for better accuracy.\n",
    "import time\n",
    "sample_size = 1\n",
    "\n",
    "image_ids = np.random.choice(dataset_val.image_ids, sample_size)\n",
    "APs = []\n",
    "\n",
    "item = 0\n",
    "start = time.time()\n",
    "print(\"Calculating mAP on {} items\".format(sample_size))\n",
    "for image_id in image_ids:\n",
    "    if(item % 10 == 0):\n",
    "        end = time.time()\n",
    "        print(\"{} percent done\".format(100*item/sample_size))\n",
    "        print(\"{} seconds per item\".format((end-start)/10))\n",
    "        start = time.time()\n",
    "    item = item+1\n",
    "    #Load image and ground truth data\n",
    "    image_org, image_meta, gt_class_id, gt_bbox, gt_mask, gt_landmark =\\\n",
    "    modellib.load_image_gt(dataset_val, inference_config, \n",
    "                           image_id, use_mini_mask=False)\n",
    "\n",
    "    molded_images = np.expand_dims(modellib.mold_image(image_org, inference_config), 0)\n",
    "    # Run object detection\n",
    "    results = model.detect([image_org], verbose=0)\n",
    "    r = results[0]\n",
    "    \n",
    "    # Compute AP\n",
    "    AP, precisions, recalls, overlaps =\\\n",
    "        utils.compute_ap(gt_bbox, gt_class_id, gt_mask,\n",
    "                         r[\"rois\"], r[\"class_ids\"], r[\"scores\"], r['masks'])\n",
    "    APs.append(AP)\n",
    "    \n",
    "print(\"mAP: \", np.mean(APs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Class Accuracy\n",
    "### Setup data tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import ast\n",
    "\n",
    "data = pd.read_csv('val_class_predictions.csv')\n",
    "preds = data.drop(columns={'Unnamed: 0'})\n",
    "\n",
    "data = pd.read_csv('val_gt.csv')\n",
    "truth = data.drop(columns={'Unnamed: 0'})\n",
    "\n",
    "#Convert the string representation of lists into actual lists\n",
    "truth['as_list'] = truth.apply(lambda x: ast.literal_eval(x['gt']),axis=1)\n",
    "\n",
    "#Add the number of clothes in each image for convenience \n",
    "truth['gt_item_count'] = truth.apply(lambda x: len(x['as_list']),axis=1)\n",
    "\n",
    "#Join the lists\n",
    "table = preds.join(truth)\n",
    "\n",
    "#Check it\n",
    "table.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single item detection\n",
    "Some statistics when there is only one article of clothing in the image to detect, for simplicitys sake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get single-item images\n",
    "single = table[table['gt_item_count'] == 1]\n",
    "\n",
    "#Pick the items from the list, otherwise we can't use groupby\n",
    "single['gt'] = single.apply(lambda x: x['as_list'][0],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each type of cloth, check what the network believes it to be on average\n",
    "average_confidence = single.groupby('gt').mean().drop(columns = {'gt_item_count'})\n",
    "\n",
    "#Use names instead of numbers for predictions\n",
    "average_confidence.columns = dataset_val.class_names\n",
    "\n",
    "#The BG column is useless\n",
    "average_confidence = average_confidence.drop(columns=\"BG\")\n",
    "\n",
    "#Use names instead of numbers for gt_classes\n",
    "average_confidence.index = average_confidence.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualize it\n",
    "sns.set()\n",
    "fig, ax = plt.subplots(figsize = (15,10))\n",
    "sns.heatmap(average_confidence, annot=True, ax = ax)\n",
    "ax.set_ylim(len(average_confidence),0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of items in each class\")\n",
    "fix, ax = plt.subplots(figsize = (15,10))\n",
    "counts = single.groupby('gt').count()[['gt_item_count']]\n",
    "counts.index = average_confidence.columns.values\n",
    "\n",
    "sns.barplot(x=counts.index, y = counts.gt_item_count)\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
